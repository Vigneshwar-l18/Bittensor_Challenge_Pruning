{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Necessary libraries\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transform object to convert the MNIST dataset images to a float tensor \n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor()])\n",
    "# Loading the train and test datasets and their loader objects with a batch size of 10\n",
    "dataset_train = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.MNIST(\"data\", train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network \n",
    "'''\n",
    "Construct a ReLU-activated neural network with four hidden layers with sizes [1000, 1000, 500, 200].\n",
    "\n",
    "'''\n",
    "\n",
    "def neural_net():\n",
    "    Model = nn.Sequential(\n",
    "            nn.Linear(28*28, 1000,bias=False),nn.ReLU(),\n",
    "            nn.Linear(1000, 1000,bias=False),nn.ReLU(),\n",
    "            nn.Linear(1000, 500,bias=False),nn.ReLU(),\n",
    "            nn.Linear(500, 200,bias=False),nn.ReLU(),\n",
    "            nn.Linear(200,10,bias=False))\n",
    "    \n",
    "    print(Model)\n",
    "\n",
    "    return Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 2.302585\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 2.302585\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# Training the model using Adam Optimizer and the loss function used is Cross Entropy Loss\n",
    "\n",
    "\n",
    "def train(model, train_loader,epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for index,(data, labels) in enumerate(train_loader):    #Training Process started\n",
    "        optimizer = optim.Adam(model.parameters(),lr=0.1)        #Learning Rate =0.1\n",
    "        optimizer.zero_grad()                                   #Setting the gradients to 0\n",
    "        #labels = labels.to(device)\n",
    "        output = model(data.view(32, -1))                      #Forward Pass\n",
    "        loss = nn.CrossEntropyLoss()                           #Loss function initialization\n",
    "        l = loss(output,labels)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        if index % 1000 == 0:                                 #Calculating the loss for each epoch\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    index * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * index / len(train_loader),\n",
    "                    l.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            \n",
    "num_epochs = 25\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model trained for 25 epochs\n",
    "torch.save(model.state_dict(), \"bittensor_trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: tensor(0.0721)\n",
      "Test Accuracy =  9.800000190734863\n"
     ]
    }
   ],
   "source": [
    "#Getting the Test Accuracies and Loss before pruning the weights (Pre-Pruning)\n",
    "\n",
    "import numpy as np\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = list(0. for i in range(10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            output = model(data.view(data.shape[0],-1))\n",
    "            predicted = torch.max(output.data, axis=1)[1]          #Converting the probabilities to predicted class\n",
    "            test_loss += loss(output, labels)\n",
    "            correct += (predicted == labels).sum()                #Counting the no. of correct predictions\n",
    "    avg_test_loss = test_loss/len(test_loader.dataset)\n",
    "    #avg_test_acc = (correct/len(test_loader.dataset)).numpy()\n",
    "    print(\n",
    "        \"\\nTest set: Average loss:\",str(avg_test_loss)\n",
    "        )\n",
    "    accuracy = float(100. * (correct/dataset_test.data.shape[0]))\n",
    "    #print(\"Test Accuracy = \",accuracy)    \n",
    "    return accuracy   \n",
    "\n",
    "test_acc = test(model, test_loader)  #Calculating the Test loss and Test Accuracy before pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_percentage = [.0, .25, .50, .60, .70, .80, .90, .95, .97, .99]\n",
    "accuracy_weight = []\n",
    "accuracy_unit = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEIGHT PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_pruning(k):\n",
    "    model = neural_net()\n",
    "    model.load_state_dict(torch.load('bittensor_trained_model.pth'))\n",
    "    layers = list(model.state_dict())\n",
    "    layer_weights = {}\n",
    "    pruned_weights =[]\n",
    "    weights = model.state_dict()\n",
    "    for l in layers[:-1]:\n",
    "        a = weights[l]\n",
    "        w = np.array(a)\n",
    "        layer_weights[l]=(rankdata(np.abs(w), method='dense') - 1).astype(int).reshape(w.shape)\n",
    "        threshold_val = np.ceil(np.max(layer_weights[l]) * k).astype(int)\n",
    "        layer_weights[l][layer_weights[l] <= threshold_val] = 0                  # 0 below threshold\n",
    "        layer_weights[l][layer_weights[l] > threshold_val] = 1                   # 1 above threshold\n",
    "#         if layer_weights[l].any() <= threshold_val:\n",
    "#             layer_weights[l] = 0\n",
    "#         if layer_weights[l].any() > threshold_val:\n",
    "#             layer_weights[l] = 0    \n",
    "        w = w * layer_weights[l]\n",
    "        a[...] = torch.from_numpy(w)\n",
    "        pruned_weights.append(data)\n",
    "    \n",
    "    pruned_weights.append(weights[layers[-1]])                      # Append the output layer weights\n",
    "\n",
    "    pruned_state_dict = OrderedDict()                                  # Store and update the weights\n",
    "    for i, w in zip(layers, pruned_weights):\n",
    "        pruned_state_dict[i] = w\n",
    "        \n",
    "    model.state_dict = pruned_state_dict\n",
    "    accuracy_weight.append(test(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIT PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "def unit_pruning(k):\n",
    "    model = neural_net()\n",
    "    model.load_state_dict(torch.load('bittensor_trained_model.pth'))\n",
    "    layers = list(model.state_dict())\n",
    "    layer_weights = {}\n",
    "    pruned_weights =[]\n",
    "    weights = model.state_dict()\n",
    "    for l in layers[:-1]:\n",
    "        a = weights[l]\n",
    "        w = np.array(a)\n",
    "        norm = linalg.norm(w, axis=0)                       \n",
    "        norm = np.tile(norm, (w.shape[0],1))            \n",
    "        layer_weights[l]=(rankdata(np.abs(w), method='dense') - 1).astype(int).reshape(w.shape)\n",
    "        threshold_val = np.ceil(np.max(layer_weights[l]) * k).astype(int)\n",
    "        layer_weights[l][layer_weights[l] <= threshold_val] = 0                  # 0 below threshold\n",
    "        layer_weights[l][layer_weights[l] > threshold_val] = 1                   # 1 above threshold\n",
    "#         if layer_weights[l].any() <= threshold_val:\n",
    "#             layer_weights[l] = 0\n",
    "#         if layer_weights[l].any() > threshold_val:\n",
    "#             layer_weights[l] = 0    \n",
    "        w = w * layer_weights[l]\n",
    "        a[...] = torch.from_numpy(w)\n",
    "        pruned_weights.append(data)\n",
    "    \n",
    "    pruned_weights.append(weights[layers[-1]])                      # Append the output layer weights\n",
    "\n",
    "    pruned_state_dict = OrderedDict()                                  # Store and update the weights\n",
    "    for i, w in zip(layers, pruned_weights):\n",
    "        pruned_state_dict[i] = w\n",
    "        \n",
    "    model.state_dict = pruned_state_dict\n",
    "    accuracy_unit.append(test(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0683)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0683)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.70000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.689998626708984\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0682)\n",
      "Test Accuracy =  36.689998626708984\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0681)\n",
      "Test Accuracy =  36.5099983215332\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0681)\n",
      "Test Accuracy =  36.5099983215332\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0684)\n",
      "Test Accuracy =  34.45000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0684)\n",
      "Test Accuracy =  34.45000076293945\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0694)\n",
      "Test Accuracy =  35.540000915527344\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0694)\n",
      "Test Accuracy =  35.540000915527344\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0878)\n",
      "Test Accuracy =  36.65999984741211\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0878)\n",
      "Test Accuracy =  36.65999984741211\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0986)\n",
      "Test Accuracy =  36.7599983215332\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=1000, bias=False)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1000, out_features=1000, bias=False)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=1000, out_features=500, bias=False)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=500, out_features=200, bias=False)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=200, out_features=10, bias=False)\n",
      ")\n",
      "\n",
      "Test set: Average loss: tensor(0.0986)\n",
      "Test Accuracy =  36.7599983215332\n"
     ]
    }
   ],
   "source": [
    "for k in k_percentage:\n",
    "    weight_pruning(k)\n",
    "    unit_pruning(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy WEIGHT =  [36.70000076293945, 36.70000076293945, 36.70000076293945, 36.70000076293945, 36.689998626708984, 36.5099983215332, 34.45000076293945, 35.540000915527344, 36.65999984741211, 36.7599983215332]\n",
      "Accuracy UNIT =  [36.70000076293945, 36.70000076293945, 36.70000076293945, 36.70000076293945, 36.689998626708984, 36.5099983215332, 34.45000076293945, 35.540000915527344, 36.65999984741211, 36.7599983215332]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy WEIGHT = \",accuracy_weight)\n",
    "print(\"Accuracy UNIT = \",accuracy_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING THE GRAPHS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = [0, 25, 50, 60, 70, 80, 90, 95, 97, 99]\n",
    "plt.title('Accuracy vs. Sparsity')\n",
    "plt.plot(k, accuracy_unit, label='Unit Pruning')\n",
    "plt.plot(k, accuracy_weight, label='Weight Pruning')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('% sparsity')\n",
    "plt.ylabel('% accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What interesting insights did you find?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The interesting insight to infer from the plot is that the neural network can be pruned from 50% - ~80% without affecting the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Do the curves differ? Why do you think that is/isn’t?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Form the plots, both the unit and weight pruning looks similar. The reason for this could be of the low observations. There would be a great clarity if there were more observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Do you have any hypotheses as to why we are able to delete so much of the network without hurting performance "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In a trained model of a very large neural network, there are possibilities that a no. of parameters in this large model are unused.Considering the amount of time a large neural network model requires to train, these parameters could be removed via pruning and thereby reducing the computations and thereby creating a compressed network\n",
    "\n",
    "Another factor that I think is the weights which are of less significance or small in activating the neurons could be a reason for the larger models.These low-significant weights could be removed via pruning and thereby not affectig the performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
